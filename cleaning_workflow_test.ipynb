{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b3ddaf",
   "metadata": {},
   "source": [
    "Note: These set of functions require the following packages:\n",
    "-pandas\n",
    "-numpy\n",
    "-statsmodel\n",
    "-typing\n",
    "-warnings\n",
    "-re\n",
    "-dataclasses\n",
    "-scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378118f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script contains the main cofiguration for each survey method\n",
    "#Also contains a lot of helper functions for plausible value usage, methods involving weights and replicate weights\n",
    "import core  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683ef919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleaner ##Contains the remove_na algorithm + adjust weights algorithm that PISA uses. Adds a method that would flatten the dataset with the given weights\n",
    "import estimation ##Contains some of the refactored estimation statistic from the repest library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2a23292",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cleaning example\n",
    "# I'm using a filtered Dataset from the 2022 students QQQ where we're left with\n",
    "# the SEA data along with the responses from ST251\n",
    "\n",
    "import pandas as pd\n",
    "from cleaner import DataCleaner\n",
    "\n",
    "df = pd.read_csv('SEA_students_QQQ_reduced.csv')\n",
    "\n",
    "#Select the variables\n",
    "learning_time_cols = [c for c in df.columns if 'ST251' in c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c9152c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial population estimate: 7,682,567 students\n"
     ]
    }
   ],
   "source": [
    "#Run this script for us to have an idea about the population estimate based on the weight\n",
    "print(f\"Initial population estimate: {df['W_FSTUWT'].sum():,.0f} students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eea141",
   "metadata": {},
   "source": [
    "NOW WE TRY THE CLEANER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5882874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ST251Q01JA    1360\n",
      "ST251Q02JA    1285\n",
      "ST251Q03JA    1438\n",
      "ST251Q04JA    1311\n",
      "ST251Q06JA    1308\n",
      "ST251Q07JA    1310\n",
      "ST251D08JA    7792\n",
      "ST251D09JA    7709\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##We start by recoding the na values\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# List of variables to clean\n",
    "target_vars = [\n",
    "    'ST251Q01JA', 'ST251Q02JA', 'ST251Q03JA', 'ST251Q04JA', \n",
    "    'ST251Q06JA', 'ST251Q07JA', 'ST251D08JA', 'ST251D09JA'\n",
    "]\n",
    "\n",
    "# Then force the variables to be numeric\n",
    "df[target_vars] = df[target_vars].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Replace the PISA-specific missing codes with NaN for these variables\n",
    "pisa_missing = [97, 98, 99, 997, 998, 999, 9997, 9998, 9999,9999997,9999998,9999999]\n",
    "df[target_vars] = df[target_vars].replace(pisa_missing, np.nan)\n",
    "\n",
    "##Check the number of missing values \n",
    "print(df[target_vars].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec5781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      variable  CNT  n_total  n_missing  pct_missing\n",
      "0   ST251Q01JA  BRN     5576         35     0.627690\n",
      "1   ST251Q02JA  BRN     5576        128     2.295552\n",
      "2   ST251Q03JA  BRN     5576         58     1.040172\n",
      "3   ST251Q04JA  BRN     5576         56     1.004304\n",
      "4   ST251Q06JA  BRN     5576         70     1.255380\n",
      "5   ST251Q07JA  BRN     5576         72     1.291248\n",
      "6   ST251D08JA  BRN     5576         69     1.237446\n",
      "7   ST251D09JA  BRN     5576         54     0.968436\n",
      "8   ST251Q01JA  IDN    13439        399     2.968971\n",
      "9   ST251Q02JA  IDN    13439        279     2.076047\n",
      "10  ST251Q03JA  IDN    13439        327     2.433217\n",
      "11  ST251Q04JA  IDN    13439        287     2.135576\n",
      "12  ST251Q06JA  IDN    13439        289     2.150458\n",
      "13  ST251Q07JA  IDN    13439        308     2.291837\n",
      "14  ST251D08JA  IDN    13439        243     1.808170\n",
      "15  ST251D09JA  IDN    13439        275     2.046283\n",
      "16  ST251Q01JA  KHM     5279        384     7.274105\n",
      "17  ST251Q02JA  KHM     5279        291     5.512408\n",
      "18  ST251Q03JA  KHM     5279        421     7.974995\n",
      "19  ST251Q04JA  KHM     5279        448     8.486456\n",
      "20  ST251Q06JA  KHM     5279        441     8.353855\n",
      "21  ST251Q07JA  KHM     5279        415     7.861337\n",
      "22  ST251D08JA  KHM     5279        400     7.577193\n",
      "23  ST251D09JA  KHM     5279        414     7.842394\n",
      "24  ST251Q01JA  MYS     7069        100     1.414627\n",
      "25  ST251Q02JA  MYS     7069        152     2.150233\n",
      "26  ST251Q03JA  MYS     7069        171     2.419013\n",
      "27  ST251Q04JA  MYS     7069        149     2.107795\n",
      "28  ST251Q06JA  MYS     7069        138     1.952186\n",
      "29  ST251Q07JA  MYS     7069        150     2.121941\n",
      "30  ST251D08JA  MYS     7069        161     2.277550\n",
      "31  ST251D09JA  MYS     7069        137     1.938039\n",
      "32  ST251Q01JA  PHL     7193        130     1.807313\n",
      "33  ST251Q02JA  PHL     7193        133     1.849020\n",
      "34  ST251Q03JA  PHL     7193        173     2.405116\n",
      "35  ST251Q04JA  PHL     7193        145     2.015849\n",
      "36  ST251Q06JA  PHL     7193        137     1.904630\n",
      "37  ST251Q07JA  PHL     7193        157     2.182678\n",
      "38  ST251D08JA  PHL     7193        156     2.168775\n",
      "39  ST251D09JA  PHL     7193        106     1.473655\n",
      "40  ST251Q01JA  SGP     6606         47     0.711474\n",
      "41  ST251Q02JA  SGP     6606         78     1.180745\n",
      "42  ST251Q03JA  SGP     6606         51     0.772025\n",
      "43  ST251Q04JA  SGP     6606         62     0.938541\n",
      "44  ST251Q06JA  SGP     6606         47     0.711474\n",
      "45  ST251Q07JA  SGP     6606         42     0.635786\n",
      "46  ST251D08JA  SGP     6606       6606   100.000000\n",
      "47  ST251D09JA  SGP     6606       6606   100.000000\n",
      "48  ST251Q01JA  THA     8495         87     1.024132\n",
      "49  ST251Q02JA  THA     8495        162     1.907004\n",
      "50  ST251Q03JA  THA     8495        142     1.671572\n",
      "51  ST251Q04JA  THA     8495        101     1.188935\n",
      "52  ST251Q06JA  THA     8495         87     1.024132\n",
      "53  ST251Q07JA  THA     8495         86     1.012360\n",
      "54  ST251D08JA  THA     8495         86     1.012360\n",
      "55  ST251D09JA  THA     8495         71     0.835786\n",
      "56  ST251Q01JA  VNM     6068        178     2.933421\n",
      "57  ST251Q02JA  VNM     6068         62     1.021753\n",
      "58  ST251Q03JA  VNM     6068         95     1.565590\n",
      "59  ST251Q04JA  VNM     6068         63     1.038233\n",
      "60  ST251Q06JA  VNM     6068         99     1.631510\n",
      "61  ST251Q07JA  VNM     6068         80     1.318392\n",
      "62  ST251D08JA  VNM     6068         71     1.170073\n",
      "63  ST251D09JA  VNM     6068         46     0.758075\n"
     ]
    }
   ],
   "source": [
    "##Then check the na distribution\n",
    "cleaner = DataCleaner(df, survey='PISA') # Set the dataframe df for the object cleaner and use the default configuration for PISA\n",
    "missing_summary = cleaner.summarize_missingness(columns=learning_time_cols, by='CNT')\n",
    "\n",
    "print(missing_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Cleaning Summary:\n",
      "  Total rows: 59,725\n",
      "  Rows with missing values: 9,957 (16.67%)\n",
      "  Rows retained: 49,768 (83.33%)\n",
      "  Weights adjusted within: CNT\n",
      "\n",
      "  Weight adjustment factors by group:\n",
      "    BRN: 1.0000x\n",
      "    IDN: 1.0000x\n",
      "    KHM: 1.0000x\n",
      "    MYS: 1.0000x\n",
      "    PHL: 1.0000x\n",
      "    THA: 1.0000x\n",
      "    VNM: 1.0000x\n",
      "Original Weighted Total: 7,682,567.21\n",
      "Adjusted Weighted Total: 7,640,609.37\n",
      "Difference: 41957.8350\n"
     ]
    }
   ],
   "source": [
    "### Then we try the cleaning function\n",
    "\n",
    "# We group by CNT to ensure the population total for EACH country remains stable, we can use the STRATUM grouping as well if we're going over local data\n",
    "df_cleaned = cleaner.remove_na_adjust_weights(\n",
    "    columns=learning_time_cols, \n",
    "    by='CNT', \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Compare Population Totals\n",
    "old_total = df['W_FSTUWT'].sum()\n",
    "new_total = df_cleaned['W_FSTUWT'].sum()\n",
    "\n",
    "print(f\"Original Weighted Total: {old_total:,.2f}\")\n",
    "print(f\"Adjusted Weighted Total: {new_total:,.2f}\")\n",
    "print(f\"Difference: {old_total - new_total:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c8b98",
   "metadata": {},
   "source": [
    "A 40k+ difference was unexpected. This signals that some of the groups have NA values entirely on a particular columns. To explore this, imma try to expand the dataframe on missing summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b95c99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      variable  CNT  n_total  n_missing  pct_missing\n",
      "0   ST251Q01JA  BRN     5576         35     0.627690\n",
      "1   ST251Q02JA  BRN     5576        128     2.295552\n",
      "2   ST251Q03JA  BRN     5576         58     1.040172\n",
      "3   ST251Q04JA  BRN     5576         56     1.004304\n",
      "4   ST251Q06JA  BRN     5576         70     1.255380\n",
      "5   ST251Q07JA  BRN     5576         72     1.291248\n",
      "6   ST251D08JA  BRN     5576         69     1.237446\n",
      "7   ST251D09JA  BRN     5576         54     0.968436\n",
      "8   ST251Q01JA  IDN    13439        399     2.968971\n",
      "9   ST251Q02JA  IDN    13439        279     2.076047\n",
      "10  ST251Q03JA  IDN    13439        327     2.433217\n",
      "11  ST251Q04JA  IDN    13439        287     2.135576\n",
      "12  ST251Q06JA  IDN    13439        289     2.150458\n",
      "13  ST251Q07JA  IDN    13439        308     2.291837\n",
      "14  ST251D08JA  IDN    13439        243     1.808170\n",
      "15  ST251D09JA  IDN    13439        275     2.046283\n",
      "16  ST251Q01JA  KHM     5279        384     7.274105\n",
      "17  ST251Q02JA  KHM     5279        291     5.512408\n",
      "18  ST251Q03JA  KHM     5279        421     7.974995\n",
      "19  ST251Q04JA  KHM     5279        448     8.486456\n",
      "20  ST251Q06JA  KHM     5279        441     8.353855\n",
      "21  ST251Q07JA  KHM     5279        415     7.861337\n",
      "22  ST251D08JA  KHM     5279        400     7.577193\n",
      "23  ST251D09JA  KHM     5279        414     7.842394\n",
      "24  ST251Q01JA  MYS     7069        100     1.414627\n",
      "25  ST251Q02JA  MYS     7069        152     2.150233\n",
      "26  ST251Q03JA  MYS     7069        171     2.419013\n",
      "27  ST251Q04JA  MYS     7069        149     2.107795\n",
      "28  ST251Q06JA  MYS     7069        138     1.952186\n",
      "29  ST251Q07JA  MYS     7069        150     2.121941\n",
      "30  ST251D08JA  MYS     7069        161     2.277550\n",
      "31  ST251D09JA  MYS     7069        137     1.938039\n",
      "32  ST251Q01JA  PHL     7193        130     1.807313\n",
      "33  ST251Q02JA  PHL     7193        133     1.849020\n",
      "34  ST251Q03JA  PHL     7193        173     2.405116\n",
      "35  ST251Q04JA  PHL     7193        145     2.015849\n",
      "36  ST251Q06JA  PHL     7193        137     1.904630\n",
      "37  ST251Q07JA  PHL     7193        157     2.182678\n",
      "38  ST251D08JA  PHL     7193        156     2.168775\n",
      "39  ST251D09JA  PHL     7193        106     1.473655\n",
      "40  ST251Q01JA  SGP     6606         47     0.711474\n",
      "41  ST251Q02JA  SGP     6606         78     1.180745\n",
      "42  ST251Q03JA  SGP     6606         51     0.772025\n",
      "43  ST251Q04JA  SGP     6606         62     0.938541\n",
      "44  ST251Q06JA  SGP     6606         47     0.711474\n",
      "45  ST251Q07JA  SGP     6606         42     0.635786\n",
      "46  ST251D08JA  SGP     6606       6606   100.000000\n",
      "47  ST251D09JA  SGP     6606       6606   100.000000\n",
      "48  ST251Q01JA  THA     8495         87     1.024132\n",
      "49  ST251Q02JA  THA     8495        162     1.907004\n",
      "50  ST251Q03JA  THA     8495        142     1.671572\n",
      "51  ST251Q04JA  THA     8495        101     1.188935\n",
      "52  ST251Q06JA  THA     8495         87     1.024132\n",
      "53  ST251Q07JA  THA     8495         86     1.012360\n",
      "54  ST251D08JA  THA     8495         86     1.012360\n",
      "55  ST251D09JA  THA     8495         71     0.835786\n",
      "56  ST251Q01JA  VNM     6068        178     2.933421\n",
      "57  ST251Q02JA  VNM     6068         62     1.021753\n",
      "58  ST251Q03JA  VNM     6068         95     1.565590\n",
      "59  ST251Q04JA  VNM     6068         63     1.038233\n",
      "60  ST251Q06JA  VNM     6068         99     1.631510\n",
      "61  ST251Q07JA  VNM     6068         80     1.318392\n",
      "62  ST251D08JA  VNM     6068         71     1.170073\n",
      "63  ST251D09JA  VNM     6068         46     0.758075\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11194b08",
   "metadata": {},
   "source": [
    "yea.. the intuition is correct. So we either remove SGP from this analysis or remove that particular column. I'm taking the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f73c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['ST251D08JA', 'ST251D09JA'])\n",
    "\n",
    "\n",
    "## then update the learning_time_cols with this adjustment\n",
    "learning_time_cols = [c for c in df.columns if 'ST251' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5066d6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Cleaning Summary:\n",
      "  Total rows: 59,725\n",
      "  Rows with missing values: 3,245 (5.43%)\n",
      "  Rows retained: 56,480 (94.57%)\n",
      "  Weights adjusted within: CNT\n",
      "\n",
      "  Weight adjustment factors by group:\n",
      "    BRN: 1.0000x\n",
      "    IDN: 1.0000x\n",
      "    KHM: 1.0000x\n",
      "    MYS: 1.0000x\n",
      "    PHL: 1.0000x\n",
      "    SGP: 1.0000x\n",
      "    THA: 1.0000x\n",
      "    VNM: 1.0000x\n",
      "Original Weighted Total: 7,682,567.21\n",
      "Adjusted Weighted Total: 7,682,567.21\n",
      "Difference: 0.0000\n"
     ]
    }
   ],
   "source": [
    "#Redoing the cleaning\n",
    "df_cleaned = cleaner.remove_na_adjust_weights(\n",
    "    columns=learning_time_cols, \n",
    "    by='CNT', \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Validation: Compare Population Totals\n",
    "old_total = df['W_FSTUWT'].sum()\n",
    "new_total = df_cleaned['W_FSTUWT'].sum()\n",
    "\n",
    "print(f\"Original Weighted Total: {old_total:,.2f}\")\n",
    "print(f\"Adjusted Weighted Total: {new_total:,.2f}\")\n",
    "print(f\"Difference: {old_total - new_total:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408e738",
   "metadata": {},
   "source": [
    "Cool. So the code's working fine as long as there are no perfectly 100% NA rows or columns.\n",
    "\n",
    "There's this other function for flattening that i think would be useful for you guys who are not yet comfortable with dealing with weights. It simply flattens the dataset using the weights per row. WARNING. If we take SEA data for example, we should expect a dataset with around 7.6M rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9efd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "##For this demo, imma take 10% of PH data from the previous df\n",
    "ph_df = df[df['CNT'] == 'PHL'].copy()\n",
    "#I'm also taking around 30% of the data (just to be safe for in memory storage). I'm adding a specific seed to make sure that it's redoable\n",
    "ph_sample = ph_df.sample(frac=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476a6a9",
   "metadata": {},
   "source": [
    "Now we apply the workflow for cleaning. SInce we're using the earlier dataset, the learning cols variable is still intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08abdd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     variable  CNT  n_total  n_missing  pct_missing\n",
      "0  ST251Q01JA  PHL     2158         35     1.621872\n",
      "1  ST251Q02JA  PHL     2158         32     1.482854\n",
      "2  ST251Q03JA  PHL     2158         47     2.177943\n",
      "3  ST251Q04JA  PHL     2158         40     1.853568\n",
      "4  ST251Q06JA  PHL     2158         36     1.668211\n",
      "5  ST251Q07JA  PHL     2158         44     2.038925\n"
     ]
    }
   ],
   "source": [
    "##Again, we set the new dataset in our cleaner function then check the na distribution\n",
    "cleaner = DataCleaner(ph_sample, survey='PISA') # set the dataframe and use PISA configuration\n",
    "\n",
    "## Then identify the NAs on our target variables\n",
    "target_vars = [\n",
    "    'ST251Q01JA', 'ST251Q02JA', 'ST251Q03JA', 'ST251Q04JA', \n",
    "    'ST251Q06JA', 'ST251Q07JA'\n",
    "]\n",
    "\n",
    "# Force only these specific columns to be numeric\n",
    "ph_sample[target_vars] = ph_sample[target_vars].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Replace the PISA-specific missing codes with NaN for these variables\n",
    "pisa_missing = [97, 98, 99, 997, 998, 999, 9997, 9998, 9999,9999997,9999998,9999999]\n",
    "ph_sample[target_vars] = ph_sample[target_vars].replace(pisa_missing, np.nan)\n",
    "\n",
    "# Check the missingness\n",
    "missing_summary = cleaner.summarize_missingness(columns=learning_time_cols, by='CNT')\n",
    "\n",
    "\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "071c2283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Cleaning Summary:\n",
      "  Total rows: 2,158\n",
      "  Rows with missing values: 102 (4.73%)\n",
      "  Rows retained: 2,056 (95.27%)\n",
      "  Weights adjusted within: CNT\n",
      "\n",
      "  Weight adjustment factors by group:\n",
      "    PHL: 1.0000x\n",
      "Original Weighted Total: 534,252.49\n",
      "Adjusted Weighted Total: 534,252.49\n",
      "Difference: 0.0000\n"
     ]
    }
   ],
   "source": [
    "## Since there are no issues like 100% missingness, we can proceed by applying our NA removal+weight adjustment alogrithm\n",
    "#Redoing the cleaning\n",
    "df_cleaned_ph_sample = cleaner.remove_na_adjust_weights(\n",
    "    columns=learning_time_cols, \n",
    "    by='CNT', \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Validation: Compare Population Totals\n",
    "old_total = ph_sample['W_FSTUWT'].sum()\n",
    "new_total = df_cleaned_ph_sample['W_FSTUWT'].sum()\n",
    "\n",
    "print(f\"Original Weighted Total: {old_total:,.2f}\")\n",
    "print(f\"Adjusted Weighted Total: {new_total:,.2f}\")\n",
    "print(f\"Difference: {old_total - new_total:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0e63b",
   "metadata": {},
   "source": [
    "Since there are no issues with the adjustment (But i think i should've grouped them by STRATUM to at least achieve that balance within regions, thing is i forgot to include them in the original dataset for this demonstration), we can now try to flatten this and expect a 500k+ rows dataset that can easily be explored without weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d032b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flattening Data:\n",
      "  Original rows: 2,158\n",
      "  Expected output rows: 534,266\n",
      "  Expansion factor: 247.57x\n",
      "  Final rows: 534,266\n"
     ]
    }
   ],
   "source": [
    "### apply the flatten function\n",
    "flat_ph_df = cleaner.flatten_with_weights() ## i think we should them using STRATUM or CNT level for International level analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
